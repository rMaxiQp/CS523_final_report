\section{Introduction}
\label{sec:introduction}

The advancement of LLMs has catalyzed the emergence of AI-driver operations (AIOps). These systems promise significant efficiency gains by automating complicated operations, including fault identification, localization, and mitigation. However, this increasing autonomy introduces critical safety concerns, particularly when LLM agents are empowered to execute system-modifying commands in production environments.

For cloud infrastructures like Kubernetes systems, an AI agent with unrestricted access to the complex environment can potentially execute destructive commands that extend beyond the intended scope of fault and make unwarranted modifications to system components intended to remain immutable. 

The need for effective safety guardrails represents a critical research challenge at the intersection of AI safety and systems reliability. Balancing automation benefits with the necessary safety controls is essential for the responsible adoption of AIOps.

In the experimental context, a "bad" agent output is defined as: (1) operations that modify elements we did not inject faults into (scope violations) and (2) commands that alter the state of components meant to remain immutable (unwarranted modifications).

To address these concerns, we developed a framework centered around "mutable elements". These are resources affected by fault injection that require modification during mitigation. This approach enables fine-grained security control by establishing clear boundaries for permissible actions. In addition, we implemented fault-specific command restrictions that dynamically limit the actions available to the LLM agent based on the particular type of fault injected. This prevents the use of overly destructive operations in the environment that could compromise the integrity of the system.

In this paper, we attempt to address the emerging field of AIOps by providing concrete mechanisms to contain the potential risks of autonomous system operations. Through case studies across multiple fault scenarios, we quantify the effectiveness of this approach using metrics such as violation rates, tracking instances where agents attempt to modify immutable objects or execute overly destructive commands, demonstrating improvements in operational safety without compromising remediation capabilities.
