\section{Introduction}
\label{sec:introduction}

The advancement of LLMs has catalyzed the emergence of AI-driver operations (AIOps). These systems promise significant efficiency gains by automating complicated operations, including fault identification, localization, and mitigation. However, this increasing autonomy introduces critical safety concerns, particularly when LLM agents are empowered to execute system-modifying commands in production environments.

An AI agent with unrestricted access to the complex environment has the freedom of executing destructive commands. For example, an AI agent removes the whole Kubernetes cluster with the intention of fixing a pod's port configuration. On the extreme case, one may verify all operations that the agent generates, which penalizes the operation efficiency. Balancing automation benefits with the necessary safety controls is essential for the responsible adoption of AIOps, and the need for effective safety guardrails represents a critical research challenge at the intersection of AI safety and systems reliability.

In the above example, there are two issues that lead to the catastrophic outcome. First, the AI agent has the freedom to execute any command. Second, the AI agent modifies functioning resources. By treating functioning resources as immutable, the AI agent's scope is clear. It should only modify limited resources, namely \textit{mutable elements}, using restricted operations. We classify mutable elements as resources affected by the fault that require modification during mitigation, and restricted operations as a subset of allowed operations known to ensure system integrity. This approach enables fine-grained safety control and provides agents maximum freedom with guarantee of safe operations. We put this concept into practice.

In this paper, we present lightweight safety guardrails, built on top of AIOpsLab \cite{shetty2024aiagents}, providing concrete mechanisms to contain the potential risks of autonomous system operations. Through case studies across multiple fault scenarios, we quantify the effectiveness of this approach using metrics such as violation rates, tracking instances where agents attempt to modify immutable objects or execute overly destructive commands, demonstrating improvements in operational safety without compromising remediation capabilities.
