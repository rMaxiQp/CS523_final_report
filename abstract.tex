\begin{abstract}

AI for IT operations (AIOps) aims to automate complex operational tasks, such as fault mitigation, to reduce human workload and minimize customer impact. Recent advances in Large Language Models (LLMs) revolutionize AIOps via automation on general cases, e.g. recover a cluster failure. However, due to their black box nature, LLMs may generate unintentional yet destructive operations. Reliability is a critical issue needs to be addressed before putting AI Agents inside of production environment. Though the general question is hard to answer, we can provide an answer in constrained environment. This paper presents a lightweight safety guardrail embedded in AIOpsLab to measure Agents' reliability.

\end{abstract}
